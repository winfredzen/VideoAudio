# VideoAudio
入门文章

+ [零基础入门：实时音视频技术基础知识全面盘点](http://www.52im.net/thread-3079-1-1.html)
+ [即时通讯音视频开发（十九）：零基础，史上最通俗视频编码技术入门](http://www.52im.net/thread-2840-1-1.html)

知识点

+ libV4L摄像头YUV数据采集
+ X264编码H.264数据
+ 网络Socket通信协议基础，TCP/IP协议栈
+ TCP建立连接过程，阻塞or非阻塞套接字编程
+ 数据包的分发，私有协议的定义、协议头、组包
+ 客户端登录请求，命令通道、数据通道建立，数据接收，解码，渲染
+ TCP/IP协议栈底层实现，如何在底层实现可靠的数据传输，网络拥塞、抖动、丢包重传机制
+ FFmpeg
+ OpenGL ES渲染 shader
+ 音视频采集
+ 分析H.264 NALU数据的I帧、P帧、B帧，进行拆分，得到SPS、PPS关键数据。关键帧、非关键帧
+ MP4容器分析，AAC格式分析，AAC编码，实时转换
+ 用FFmpeg实时写AAC音频流，实时写视频流



## 基础

### RGB&YUV

参考：

+ [RGB and YUV](https://www.cnblogs.com/yunlambert/p/11234971.html)



> 为什么还需要`YUV`和`RGB`两种色彩模型呢？因为对于图像显示器来说，它是通过`RGB`模型来显示图像的。而在传输图像数据时是使用`YUV`模型的，因为`YUV`模型可以节省带宽。所以就需要采集图像时将`RGB`模型转换到`YUV`模型，显示时再将`YUV`模型转换为`RGB`模型。

YUV的解释参考维基百科[YUV](https://zh.wikipedia.org/wiki/YUV)

> `YUV`中的`Y`表示的是明亮度（Luminance、Luma），`UV`表示的是色度、浓度（Chrominance、Chroma）
>
> `YCbCr`则是用来描述数字的影像信号，适合视频与图片压缩以及传输，例如`MPEG`、`JPEG`
>
> 常见YUV有很多规格，例如YUV444，YUV422和YUV420，后面的数字是表示采样的比例。其中YUV420是FFmpeg里最常用的，因为最省资源。
>
> - 4:4:4表示完全取样。
> - 4:2:2表示2:1的水平取样，垂直完全采样。
> - 4:2:0表示2:1的水平取样，垂直2:1采样
>
> 具体解释可参考：
>
> + [刻意练习FFmpeg系列：颜色和像素](https://zhuanlan.zhihu.com/p/137836227)
> + [视频压缩与编解码的基本原理](https://zhuanlan.zhihu.com/p/67305755)
>
> ![010](https://github.com/winfredzen/VideoAudio/blob/main/images/010.png)





### 音频相关参数

+ 采样率 - 一秒钟的采样数，常见44100，表示一秒钟的采样数据是44100个

+ 位宽 - 也叫采样位深，音频的位深度决定动态范围。我们常见的16bit，可以记录大概96分贝的动态范围

+ 通道数 - 一般是单通道或者双通道

+ 码率 - 又称为比特率，是指一个音频流中一秒钟能通过的数据量。比如128kbps，ps为每秒，kb为千位，128kbps表示的是一秒钟能传递的数据量是128千位。对于格式相同的文件来说，码率越大，音质越好

  码率 = 采样率 * 位宽 * 通道数 

+ 音频文件的大小 = 时长 * 码率 / 1024 / 8 = xx M



### 从JPEG到H.264

音视频编码卡的视频编码算法从JPEG发展到MPEG-1、MPEG-2、MPEG-4和H.264。JPEG是一种著名的图像压缩算法，主要应用于静态图像压缩，如果把它应用在运动图像上，就是通常所说的Motion-JPEG。

JPEG相当于MPEG的帧内压缩，因为没有去处时域上的冗余，所以在保证一定图像质量的同时，压缩比不高，通常只有10-30倍，但它有一个固定的优点，就是延迟在40ms，实时性很好，所以在某些特殊的场合任然可以看到它的踪影

MPEG-1为追求更高的压缩效率，更注重去除图像系列的时间冗余度





### H.264编码原理I／B／P帧 

**I帧**:**帧内编码帧** ，I帧表示关键帧，你可以理解为这⼀帧画⾯的完整保留；解码时只需要本帧数据就可以完成（因为包含完整画⾯）

特点：

+ 它是⼀个全帧压缩编码帧。它将全帧图像信息进⾏JPEG压缩编码及传输;
+ 解码时仅⽤I帧的数据就可重构完整图像
+ I帧描述了图像背景和运动主体的详情
+ I帧不需要参考其他画⾯⽽⽣成
+ I帧是P帧和B帧的参考帧(其质量直接影响到同组中以后各帧的质量)
+ I帧是帧组GOP的基础帧(第⼀帧),在⼀组中只有⼀个I帧
+ I帧不需要考虑运动⽮量
+ I帧所占数据的信息量⽐较⼤



**P帧**:**前向预测编码帧**。P帧表示的是这⼀帧跟之前的⼀个关键帧（或P帧）的差别，解码时需要⽤之前缓存的画⾯叠加上本帧定义的差别，⽣成最终画⾯。（也就是**差别帧**，**P帧没有完整画⾯数据，只有与前⼀帧的画⾯差别的数据**）



P帧特点: 

+ P帧是I帧后⾯相隔1~2帧的编码帧; 
+ P帧采⽤运动补偿的⽅法传送它与前⾯的I或P帧的差值及运动⽮量(预测误差); 
+ 解码时必须将I帧中的预测值与预测误差求和后才能重构完整的P帧图像; 
+ P帧属于前向预测的帧间编码。它只参考前⾯最靠近它的I帧或P帧; 
+ P帧可以是其后⾯P帧的参考帧,也可以是其前后的B帧的参考帧; 
+ **由于P帧是参考帧,它可能造成解码错误的扩散**; 
+ 由于是差值传送,P帧的压缩⽐较⾼。



**B帧**:**双向预测内插编码帧**。B帧是双向差别帧，也就是B帧记录的是本帧与前后帧的差别（具体⽐较复杂，有4种情况，但我这样说简单些），换⾔之，要解码B帧，不仅要取得之前的缓存画⾯，还要解码之后的画⾯，通过前后画⾯的与本帧数据的叠加取得最终的画⾯。B帧压缩率⾼，但是解码时CPU会⽐较累。

B帧以前⾯的I或P帧和后⾯的P帧为参考帧,“找出”B帧“某点”的预测值和两个运动⽮量,并取预测差值和运动⽮量传送。接收端根据运动⽮量在两个参考帧中“找出(算出)”预测值并与差值求和,得到B帧“某点”样值,从⽽可得到完整的B帧。

B帧特点

+ B帧是由前⾯的I或P帧和后⾯的P帧来进⾏预测的; 
+ B帧传送的是它与前⾯的I或P帧和后⾯的P帧之间的预测误差及运动⽮量; 
+ B帧是双向预测编码帧; 
+ B帧压缩⽐最⾼,因为它只反映丙参考帧间运动主体的变化情况,预测⽐较准确; 
+ **B帧不是参考帧,不会造成解码错误的扩散**



#### PTS DTS

参考：

+ [图解DTS和PTS](https://www.cnblogs.com/linyilong3/p/9940230.html)
+ [理解音视频 PTS 和 DTS](https://www.cnblogs.com/samirchen/p/7071824.html)

DTS、PTS 的概念如下所述：

- DTS（Decoding Time Stamp）：即**解码时间戳**，这个时间戳的意义在于告诉播放器该在什么时候解码这一帧的数据。
- PTS（Presentation Time Stamp）：即**显示时间戳**，这个时间戳用来告诉播放器该在什么时候显示这一帧的数据

需要注意的是：虽然 DTS、PTS 是用于指导播放端的行为，但它们是在编码的时候由编码器生成的。

当视频流中没有 B 帧时，通常 DTS 和 PTS 的顺序是一致的。但如果有 B 帧时，就回到了我们前面说的问题：解码顺序和播放顺序不一致了。

![006](https://github.com/winfredzen/VideoAudio/blob/main/images/006.png)



**直播如何实现秒开？**

> 没有使用B帧



#### GOP

参考：

+ [简析H264编码中的GOP](https://blog.csdn.net/feitsg/article/details/105619729)

`GOP`即`Group of Pictures`，意思是画面组，一个`GOP`就是一组连续的画面。从⼀个`I帧`到下⼀个`I帧`之前的⼀组视频帧

![007](https://github.com/winfredzen/VideoAudio/blob/main/images/007.png)



#### IDR

`IDR（Instantaneous Decoding Refresh）`: 即时解码刷新。

**I帧和IDR帧的区别** 

`IDR`: 在H.264中，图像以序列为单位进⾏组织。⼀个序列的第⼀个图像叫做 IDR 图像（⽴即刷新图像），IDR 图像都是 I 帧图像。H.264 引⼊ IDR 图像是为了解码的重同步，当解码器解码到 IDR 图像时，⽴即将参考帧队列清空，将已解码的数据全部输出或抛弃，重新查找参数集，开始⼀个新的序列。这样，如果前⼀个序列出现重⼤错误，在这⾥可以获得重新同步的机会。IDR图像之后的图像永远不会使⽤IDR之前的图像的数据来解码。 **IDR 图像⼀定是 I 图像，但 I 图像不⼀定是 IDR 图像**。I帧之后的图像有可能会使⽤I帧之前的图像做运动参考。

**I和IDR帧都是使⽤帧内预测的**。它们都是同⼀个东⻄⽽已,在编码和解码中为了⽅便，要⾸个I帧和其他I帧区别开，所以才把第⼀个⾸个I帧叫IDR，这样就⽅便控制编码和解码流程。IDR帧的作⽤是⽴刻刷新,使错误不致传播,从IDR帧开始,重新算⼀个新的序列开始编码。⽽I帧不具有随机访问的能⼒，这个功能是由IDR承担。IDR会导致DPB（DecodedPictureBuffffer参考帧列表——这是关键所在）清空，⽽I不会。IDR图像⼀定是I图像，但I图像不⼀定是IDR图像。

⼀个序列中可以有很多的I图像，I图像之后的图像可以引⽤I图像之间的图像做运动参考

对于IDR帧来说，在IDR帧之后的所有帧都不能引⽤任何IDR帧之前的帧的内容，与此相反，对于普通的I帧来说，位于其之后的B-和P-帧可以引⽤位于普通I帧之前的I帧。从随机存取的视频流中，播放器永远可以从⼀个IDR帧播放，因为在它之后没有任何帧引⽤之前的帧。但是，不能在⼀个没有IDR帧的视频中从任意点开始播放，因为后⾯的帧总是会引⽤前⾯的帧。



#### x264编码举例

+ 从摄像头获取YUV数据。yuv数据⼤⼩是RGB原始数据的`width*height*3/2`。 

+ RGB需要转换成标准的`YUV420`格式才可以输⼊到编码器进⾏编码。
+ 图像组(GOP序列),每组内各帧图像定义为三种类型,即I帧、B帧和P帧;定义好之后以I帧做为基础帧,以I帧预测P帧,再由I帧和P帧预测B帧，最后将I帧数据与预测的差值信息（p帧b帧）进⾏存储和传输。
+ H264中图像以序列GOP为单位进⾏组织，以I帧开始，到下⼀个I帧结束。
+ ⼀个序列的第⼀个图像叫做 IDR 图像（⽴即刷新图像），IDR 图像都是 I 帧图像。
+ H.264 引⼊ IDR 图像是为了解码的重同步，当解码器解码到 IDR 图像时，⽴即将参考帧队列清空，将已解码的数据全部输出或抛弃，重新查找参数集，开始⼀个新的序列。这样，如果前⼀个序列出现重⼤错误，在这⾥可以获得重新同步的机会。

+ ⼀个序列就是⼀段内容差异不太⼤的图像编码后⽣成的⼀串数据流。

+ 当运动变化较少时，⼀个序列可以很⻓，因为运动变化少就代表图像画⾯的内容变动很⼩，所以就可以编⼀个I帧，然后⼀直P帧、B帧。
+ 运动变化多时，可能⼀个序列就⽐较短了，⽐如就包含⼀个I帧和3、4个P帧。此⼩段内容摘⾃H.264编码原理以及I帧B帧P帧



#### H264中NALU sps pps IDR帧 

参考：

+ [H264简介](https://www.cnblogs.com/vczf/p/13557781.html)
+ [深入浅出理解视频编码H264结构（内涵福利）](https://www.jianshu.com/p/9522c4a7818d)
+ [H.264/H265码流解析](https://www.cnblogs.com/wujianming-110117/p/12722286.html)

什么是NALU？ 

H264码流可以分为两层，VCL层和NAL层，NAL的全称是`Network abstraction layer`,叫⽹络抽象层，它保存了H264相关的参数信息和图像信息，NAL层由多个单元NALU组成,NALU由了NALU头（00 00 00 01或者00 00 01）、sps(序列参数集)、pps(图像参数集合)、slice、sei、IDR帧、I帧（在图像运动变化较少时，I帧后⾯是7个P帧，如果图像运动变化⼤时，⼀个序列就短了，I帧后⾯可能是3个或者4个P帧）、P帧、B帧等数据。sps、pps、I帧、P帧在NALU中的关系和nalu type判断

⼀个完整的NALU单元结构图如下：

![008](https://github.com/winfredzen/VideoAudio/blob/main/images/008.png)



#### H.264四种画质级别

+ BP-Baseline Profifile：基本画质。⽀持I/P 帧，只⽀持⽆交错（Progressive）和CAVLC； 
+ EP-Extended profifile：进阶画质。⽀持I/P/B/SP/SI 帧，只⽀持⽆交错（Progressive）和CAVLC； 
+ MP-Main profifile：主流画质。提供I/P/B 帧，⽀持⽆交错（Progressive）和交错（Interlaced），也⽀持CAVLC 和CABAC 的⽀持；
+ HP-High profifile：⾼级画质。在main Profifile 的基础上增加了8x8内部预测、⾃定义量化、



